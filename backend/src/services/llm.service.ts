// ===== JanSathi AI â€” LLM Service =====
// Handles all LLM interaction: completion calls and demo fallbacks.
// Abstracted behind ILLMProvider so the underlying SDK (Groq, OpenAI,
// Anthropic) can be swapped without touching chat.service.ts.

import Groq from "groq-sdk";
import { LLM, VALIDATION, type ModeName } from "../config/env";
import { SYSTEM_PROMPTS } from "../config/prompts";
import logger from "../utils/logger";

// â”€â”€ Provider Interface â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Any LLM provider (OpenAI, Anthropic, local Llamaâ€¦) must implement this.
// Swap the active provider by changing the export at the bottom of this file.
export interface ILLMProvider {
    generateResponse(input: LLMInput): Promise<LLMOutput>;
}

// â”€â”€ Shared I/O Types â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export interface LLMInput {
    mode: ModeName;
    context: string;
    message: string;
    conversationHistory: { role: "user" | "assistant"; content: string }[];
    language: "hi" | "en";
}

export interface LLMOutput {
    content: string;
    durationMs: number;
    isDemo: boolean;
}

// â”€â”€ Demo Responses (when no API key) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const DEMO_RESPONSES: Record<ModeName, string[]> = {
    janseva: [
        "ğŸ›ï¸ PM Awas Yojana ke liye aapki eligibility check karte hain.\n\n**Patrta ke liye sharaten:**\nâ€¢ Aapki saalana aamdani â‚¹3 lakh se kam ho\nâ€¢ Aapke paas pehle se pucca ghar na ho\n\nğŸ‘‰ Kya aap apni aamdani aur category bata sakte hain?",
    ],
    janshiksha: [
        "ğŸŒ± **Photosynthesis ko aise samjho:**\n\nJaise humein khana khane se energy milti hai, waise hi **paudhon ko suraj ki roshni se energy milti hai!**\n\nğŸ“š Samajh aaya? Koi aur sawal?",
    ],
    jankrishi: [
        "ğŸŒ¾ **Gehun mein Pila Rust (Yellow Rust) ki samasyaa:**\n\nâš ï¸ **URGENT: Turant action lein!**\n\n**ğŸ’Š Ilaj:**\n1ï¸âƒ£ Propiconazole (Tilt 25 EC) â€” 1 ml/litre paani\n2ï¸âƒ£ Neem tel (5ml/litre) spray",
    ],
    janvyapar: [
        "ğŸ¯ **Aapke shahad ka professional listing:**\n\nğŸ’° **Suggested Price:** 500g â†’ â‚¹350-450\n\nğŸ›’ WhatsApp Business download karein!",
    ],
    jankaushal: [
        "ğŸ“„ **Resume banana shuru karte hain!**\n\nMujhe ye jaankari dijiye:\n1ï¸âƒ£ Naam\n2ï¸âƒ£ Phone\n3ï¸âƒ£ Padhai\n4ï¸âƒ£ Skills\n\nğŸ¯ Aap bolo, main likhta hoon!",
    ],
};

function getDemoResponse(mode: ModeName): string {
    const responses = DEMO_RESPONSES[mode];
    return responses[Math.floor(Math.random() * responses.length)];
}

// â”€â”€ Groq Implementation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// To swap providers, create a new class (e.g. `OpenAILLMProvider`)
// that implements `ILLMProvider` and change the export below.
class GroqLLMProvider implements ILLMProvider {
    private client: Groq;

    constructor() {
        this.client = new Groq({ apiKey: LLM.apiKey || "" });
    }

    async generateResponse(input: LLMInput): Promise<LLMOutput> {
        const startTime = Date.now();

        // If no API key is configured, return a canned demo response
        if (!LLM.isAvailable) {
            return {
                content: getDemoResponse(input.mode),
                durationMs: Date.now() - startTime,
                isDemo: true,
            };
        }

        const systemPrompt = SYSTEM_PROMPTS[input.mode].replace("{context}", input.context);

        const llmMessages = [
            { role: "system" as const, content: systemPrompt },
            ...input.conversationHistory.slice(-VALIDATION.maxConversationHistory).map((msg) => ({
                role: msg.role as "user" | "assistant",
                content: msg.content,
            })),
            {
                role: "user" as const,
                content: `[Language: ${input.language === "hi" ? "Hindi" : "English"}]\n${input.message}`,
            },
        ];

        try {
            const completion = await this.client.chat.completions.create({
                model: LLM.model,
                messages: llmMessages,
                temperature: LLM.temperature,
                max_completion_tokens: LLM.maxTokens,
                top_p: LLM.topP,
            });

            return {
                content: completion.choices[0]?.message?.content || "Maaf kijiye, koi gadbad ho gayi.",
                durationMs: Date.now() - startTime,
                isDemo: false,
            };
        } catch (error) {
            logger.error("llm.completion.error", {
                error: error instanceof Error ? error.message : String(error),
            });
            throw error;
        }
    }
}

// â”€â”€ Active Provider â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Swap this line to change the underlying LLM without touching any
// other file. Example:
//   export const llmProvider: ILLMProvider = new OpenAILLMProvider();
export const llmProvider: ILLMProvider = new GroqLLMProvider();

// â”€â”€ Convenience wrapper (backward-compatible) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// chat.service.ts can still call generateResponse(input) directly.
export async function generateResponse(input: LLMInput): Promise<LLMOutput> {
    return llmProvider.generateResponse(input);
}
